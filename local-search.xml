<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>colmap阅读笔记</title>
    <link href="/2023/05/28/colmap-yue-du-bi-ji/"/>
    <url>/2023/05/28/colmap-yue-du-bi-ji/</url>
    
    <content type="html"><![CDATA[<h1 id="colmap论文总结"><a href="#colmap论文总结" class="headerlink" title="colmap论文总结"></a>colmap论文总结</h1><p>​colmap是一个sfm框架，主体思路为利用特征点提取出的点云，利用多副图片之间的相对运动，构建出对物体三维结构的重建，其本身为一个增量式sfm框架。</p><p>​在进行sfm重建过程中，目前认为重点是特征点提取数量的增加，其次为尽可能的减少误匹配事件的发生，方式出现计算问题。</p><p>​同时还有一个问题，在图片中是不知相机的内参矩阵的，如何完成相应任务？在进行对极几何计算时的单位？</p><h2 id="增量式sfm框架基本思路"><a href="#增量式sfm框架基本思路" class="headerlink" title="增量式sfm框架基本思路"></a>增量式sfm框架基本思路</h2><p>外点是一直存在的，要尽力减小外点对匹配的影响。</p><h3 id="相关性搜索"><a href="#相关性搜索" class="headerlink" title="相关性搜索"></a>相关性搜索</h3><p>​首先进行特征点提取，随后进行特征点匹配在图像之间进行匹配（这里方法比较多，因为匹配精度已经减少误匹配），随后利用几何验证进行匹配的验证。这里的几何验证可以使用给予对极几何的三焦点变化法。同时在重建阶段的图像注册模块可以利用此方法进行验证（F, H, E矩阵的内点的使用），此处的目的只是验证特征点匹配的正确性而不是完成图像注册。</p><p>​在这一步已经完成了特征点的提取以及匹配，最后输出了一个图。以图像为节点，以匹配结果为边的图像匹配图。</p><h3 id="增量式重建"><a href="#增量式重建" class="headerlink" title="增量式重建"></a>增量式重建</h3><p>​在这里面要确定各个特征点的具体位置，从而得到对物体的三维重建。主要方法有利用PnP进行图像注册，利用对极几何进行进行图像注册的验证（确定在坐标系下的图像位置）。以及利用三角化计算图像的深度信息，利用BA同时优化图像位姿和点云的结果。</p><p>​在最开始时，要有一步初始化，因为如三角化计算，对极几何，PnP等算法都需要一对已知的点云信息。</p><p>​随后进行增量重建：图像注册（PnP），三角化，BA，过滤外点。</p><p><strong>初始化：</strong></p><p>​同时注意到，在第一次初始化时，是利用对极几何计算的图像之间的位姿变换。这时候是无法计算出尺度信息的，同时相机的内参不知。这时我们可以假设一个相机的内参进行计算。<br>$$<br>f_x&#x3D;f_y&#x3D;c_x&#x3D;\frac{w}{2}\<br>c_y&#x3D;\frac{h}{2}<br>$$<br>​这个数据必然是不准确的，但之后有BA，在BA的过程中（其可以同时优化相机位姿以及特征点位姿）。能够不断迭代优化。</p><p>​由于后期进行图像注册时需要初始点云，所以在初始化时需要一对图像进行初始化，同时也作为之后进行增量式重建的依据。（个人理解：增量式重建的过程中，不断增加了空间中的点云的量，从而能够不断拓展点云数量，并提升重建结果）。</p><p>​在选择初始化的图像对时，需要选择一对有较多重复部分的图像对，提供尽可能稠密的点云。同时注意到后期是会进行BA优化的。</p><p>​在初始化的过程中的相机的内参矩阵的问题？之后的可以用PnP来解决。在初始化的过程中，相机内参在初始化是给了一个值作为默认，随后在优化中逐渐修正。在SFM中相机内参相对来说不是个大问题。</p><p><strong>图像注册：</strong></p><p>​利用先前已有的点云信息，对新加入的图像进行注册。方法为进行PnP。在此过程中是使用现有点云中的信息进行PnP位姿回复。（这里涉及到很多坐标系的转化问题）</p><p>​同时新加入的点云有部分虽然存在匹配，但是在现有点云库中没有相对应的位姿，此时便可以使用三角化计算出对应的位姿，同时迭代更新目前现有的点云库。</p><p>​同时注意到图像间是有对极几何计算出的位姿变化的，可以依赖这个对PnP的数据进行优化，随后也可以使用BA进行优化。</p><p><strong>这里有一种选取下一帧最优点的方法</strong></p><p><strong>三角化</strong></p><p>​输入图片都为2D图片，需要计算深度信息，此部分可以通过三角化来解决。（三角化是没有尺度信息的）此过程可以拓展点云信息</p><p><strong>多次三角化提升鲁棒性</strong></p><p><strong>BA</strong></p><p>​通过BA能够降低累计误差，能够同时优化图像位姿以及点云信息。<br>$$<br>E&#x3D;\sum_j \rho_j(\parallel \pi(P_c,X_k)-x_j \parallel_2^2)<br>$$<br>最小重投影误差，将特征点从世界坐标系向像素坐标系投影，并与真实的像素坐标作差，从而构建出对应的costFunction。此方程的自变量为costFunction中的图像位姿？</p><h2 id="colmap的改进"><a href="#colmap的改进" class="headerlink" title="colmap的改进"></a>colmap的改进</h2><h3 id="场景图的增强"><a href="#场景图的增强" class="headerlink" title="场景图的增强"></a>场景图的增强</h3><p>用多种方法确定特征点的匹配为正确的，同时计算得对极几何到的相机位姿变化是有意义的。</p><ol><li>由对极几何计算出的如H,F,E等矩阵可以作为一个验证特征点匹配正确的依据。在这里使用到的GRIC可以多图像对之间的运动进行区分。</li></ol><p>其他部分有精力慢慢更新。。。。。</p><h2 id="colmap代码"><a href="#colmap代码" class="headerlink" title="colmap代码"></a>colmap代码</h2><p>这里在暑假中自己复现过一下</p><p>本身十分不完善，有时间的话慢慢更新吧</p><p><a href="https://github.com/forgetable233/colmap_learn">https://github.com/forgetable233/colmap_learn</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>sfm</tag>
      
      <tag>reconstruction</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>nerf阅读笔记</title>
    <link href="/2023/05/28/nerf-yue-du-bi-ji/"/>
    <url>/2023/05/28/nerf-yue-du-bi-ji/</url>
    
    <content type="html"><![CDATA[<h1 id="NeRF阅读笔记"><a href="#NeRF阅读笔记" class="headerlink" title="NeRF阅读笔记"></a>NeRF阅读笔记</h1><h2 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h2><p>Neural Radiance Fields（神经辐射场）</p><p>解决问题：新视角图片生成，同时可以进行三维重建</p><p>输入：空间位置$(x, y, z, \theta, \phi)$，包括采样点的位置以及角度。</p><p>输出：$(r, g, b, \sigma)$，体素密度以及位置相关的RGB。</p><p>与SFM的对比：没有点云的存在，利用一个MLP保存的输入图片中的光学信息，并利用光学信息反推出三维模型，同时能够生成高质量的新视角图片。与传统重建方法相比，模型大小显著降低（只有一个MLP），但相较于点云或mesh而言拓展性下降。</p><p>先验输入的为相机在世界坐标系下的坐标，同时限制采样点的最近距离和最远距离。</p><h2 id="基本流程以及理解"><a href="#基本流程以及理解" class="headerlink" title="基本流程以及理解"></a>基本流程以及理解</h2><ol><li>根据相机位姿进行采样点选取，将采样点输入MLP中，得到输出</li><li>在一视线方向进行渲染，与原图片的RGB作差，构建Loss，并进行训练</li><li>完成训练后可以通过MLP进行新视角的生成</li></ol><p>nerf网络结构如下：</p><p><img src="/img/post_img/Nerf%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84.png"></p><h3 id="利用MLP进行建模"><a href="#利用MLP进行建模" class="headerlink" title="利用MLP进行建模"></a>利用MLP进行建模</h3><p>通过MLP将采样点的深度信息$(x, y, z, \theta, \phi)$，投影到$(R, G, B, \sigma)$上。</p><p><img src="/img/post_img/nerf.png"></p><p>输入的位置信息是每一个采样点的位置信息，输出的为每一个采样点的$R, G, B, \sigma$。</p><p>这里的采样点起始是随机采样的，也就是采样点在显示空间中不一定真的存在物体，这就导致nerf需要较为全面的观察角度才能达成较好的重建效果，这也导致nerf存在比较严重的虚影问题。</p><p>$R, G, B$可以看作是物体发出的光，而$\sigma$可以理解为这个位置的不透明度，也就是吸收光的能力。<strong>计算机图形学</strong>，渲染方程需要理解一下。</p><p>这里从论文中的一个图可以理解</p><p><img src="/img/post_img/%E5%85%89%E7%85%A7.png"></p><p>从图中可以看出来，尽管是同一个地方，由于观察角度不同，最后的结果也不同。</p><p>从这里可以理解，$RGB$是与观察角度相关的，$\sigma$是物体本身的性质，与观察角度无关</p><p>MLP的训练方法：根据MLP得到的结果生成输入图，并与原图利用L2损失构建损失函数，令生成图尽可能的与原图一致。</p><h3 id="进行render得到新图片"><a href="#进行render得到新图片" class="headerlink" title="进行render得到新图片"></a>进行render得到新图片</h3><p><img src="/img/post_img/%E4%BD%93%E6%B8%B2%E6%9F%93%E6%96%B9%E7%A8%8B.jpg"></p><p>上图为简单介绍的体渲染方程的流程，最后入眼的光为整个流程中的积分结果。</p><p>通过上述过程得到点的信息后，进行积分得到生成图，方程如下：</p><p>这里的体积密度$\sigma$其实可以看作光线在取样点的概率的微分，那么在计算一个点的真实颜色的过程中，需要对概率和颜色进行统一的积分<br>$$<br>r(t)&#x3D;o+td<br>\<br>C(r)&#x3D;\int_{t_n}^{t_f}T(t)\sigma(r(t))c(r(t),d), {\rm d}t \<br>T(t)&#x3D;exp(-\int_{t_n}^t\sigma(r(s)), \rm ds)<br>$$<br>积分过程没有使用确定性正积，输入为离散的体素。这是由于其只能在离散点进行查询</p><p>实际上在运算过程中的方法如下：<br>$$<br>C(t)&#x3D;\sum_{i&#x3D;1}^{N}T_i(1-exp(-\sigma_i\delta_i))c_i\<br>T_i&#x3D;exp(-\sum_{j&#x3D;i}^{i-1}\sigma_j\delta_j)\<br>\delta_i&#x3D;t_{i+1}-t_i<br>$$<br>方法为分层的抽样，在一个区间中，将其均分为$N$部分，随后随机均匀的从每一个区间中进行抽样：<br>$$<br>t_i \sim u[t_n+\frac{i-1}{N}(t_f-t_n), t_n+\frac{i}{N}(t_f-t_n)]<br>$$</p><h4 id="分层抽样的理解"><a href="#分层抽样的理解" class="headerlink" title="分层抽样的理解"></a>分层抽样的理解</h4><p>主要目的是由于在选取采样点的过程中，采样点起始是随机采样的。如果选用简单的离散化积分方法，那么采样点的选取位置其实是可以预测的，这就导致MLP只能在这些固定的位置进行优化。导致MLP无法应对任意位姿的视角生成。</p><p>这里解决方法就是使用分层抽样，增加了一个随机的过程，防止MLP只在整数位置进行优化，提升MLP效果。</p><h4 id="体渲染方程的理解"><a href="#体渲染方程的理解" class="headerlink" title="体渲染方程的理解"></a>体渲染方程的理解</h4><p>这部i分与体素渲染做对比，文章中主要是和Alpha渲染方法做对比，一个比较明显的区别是，现在这种渲染方法能够解决原有的Alpha渲染为离散空间导致的无法对任意位置进行重建的问题.</p><p>针对光照问题，利用$\sigma$可以进行建模，其为与$(x, y, z)$相关的值，可以仿照在输入一系列图片中的建模效果。<strong>如果输入的图片的光照情况完全不同，会导致nerf建模失败</strong></p><p>体素分析有一篇文章：</p><h4 id="粗细网络的意义"><a href="#粗细网络的意义" class="headerlink" title="粗细网络的意义"></a>粗细网络的意义</h4><p>主要目的是在保证采样点尽量选取在物体周围的同时，降低运算量。</p><p>流程如下:</p><p><img src="/img/post_img/%E7%B2%97%E7%BB%86%E7%BD%91%E7%BB%9C.png"></p><p>先使用粗网络大概估计哪里体素密度较高,也就是存在物体的概率较大,随后在$\sigma$高的地方增大采样密度，从而使采样点更多的集中在物体周围。</p><h2 id="传统三维重建方法"><a href="#传统三维重建方法" class="headerlink" title="传统三维重建方法"></a>传统三维重建方法</h2><p><img src="/img/post_img/%E4%BC%A0%E7%BB%9F%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA.png"></p><h2 id="Nerf的不足"><a href="#Nerf的不足" class="headerlink" title="Nerf的不足"></a>Nerf的不足</h2><p>个人的理解如下：</p><ol><li>网络拓展性差，多个网络无法合并</li><li>在训练过程中没有使用任何几何信息</li><li>只能对静态场景进行建模</li><li>当前的系统没有考虑输入图片光照信息不一致的情况</li><li>需要高精度的先验相机位姿（这里找到了几篇开创性文章，看完后在更改这里）</li><li>3D建模效果有虚影</li></ol><p>上述不足基本上都找到了一些研究文章，以后慢慢继续学习</p>]]></content>
    
    
    
    <tags>
      
      <tag>reconstruction</tag>
      
      <tag>nerf</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2023/05/26/hello-world/"/>
    <url>/2023/05/26/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
